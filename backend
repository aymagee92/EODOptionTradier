import os
import time
import logging
import random
import requests
from datetime import date, datetime, timezone

from sqlalchemy import create_engine, text
from psycopg2.extras import execute_values

# ----------------------------
# LOGGING
# ----------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
log = logging.getLogger("EOD")

# ----------------------------
# CONFIG
# ----------------------------
TICKERS = ["AAPL", "MSFT", "TSLA", "AMD"]

TRADIER_TOKEN = os.environ["TRADIER_ACCESS_TOKEN"]
PG_DSN = os.environ["PG_DSN"]

BASE_URL = "https://api.tradier.com/v1"

# Disk monitoring config (volume mount path on your droplet)
# Your Postgres data_directory is on: /mnt/volume-nyc3-01/...
# So this default should be correct; override with env if needed.
VOLUME_MOUNT_PATH = os.environ.get("VOLUME_MOUNT_PATH", "/mnt/volume-nyc3-01")
ROOT_MOUNT_PATH = os.environ.get("ROOT_MOUNT_PATH", "/")

# Optional: take a disk snapshot after EOD run completes
SNAPSHOT_AFTER_EOD = os.environ.get("SNAPSHOT_AFTER_EOD", "0") == "1"

# Performance knobs
HTTP_TIMEOUT = 20
MAX_RETRIES = 5
TICKERS_PER_BATCH = 10
SLEEP_BETWEEN_TICKERS_SECONDS = 0.0     # set >0 if you hit rate limits
SLEEP_BETWEEN_EXPIRATIONS_SECONDS = 0.0 # set >0 if you hit rate limits

HEADERS = {
    "Authorization": f"Bearer {TRADIER_TOKEN}",
    "Accept": "application/json",
}

session = requests.Session()
session.headers.update(HEADERS)

# ----------------------------
# HTTP HELPERS (fast + resilient)
# ----------------------------
def tradier_get(path: str, params: dict | None = None) -> dict:
    url = f"{BASE_URL}{path}"

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, params=params, timeout=HTTP_TIMEOUT)

            # Handle rate limiting with backoff
            if r.status_code == 429:
                sleep_s = min(30, (2 ** attempt)) + random.random()
                log.warning("429 rate limit on %s. sleep %.1fs (attempt %s/%s)", path, sleep_s, attempt, MAX_RETRIES)
                time.sleep(sleep_s)
                continue

            if r.status_code >= 400:
                # Log the body (truncated) so failures are actionable
                log.error("Tradier error %s %s params=%s body=%s", r.status_code, url, params, r.text[:800])
                r.raise_for_status()

            return r.json()

        except requests.RequestException as e:
            sleep_s = min(30, (2 ** attempt)) + random.random()
            log.warning("HTTP error on %s attempt %s/%s: %s. sleep %.1fs", path, attempt, MAX_RETRIES, e, sleep_s)
            time.sleep(sleep_s)

    raise RuntimeError(f"Tradier GET failed after {MAX_RETRIES} retries: {url} params={params}")

# ----------------------------
# TRADIER ENDPOINTS
# ----------------------------
def get_underlying_last(ticker: str) -> float | None:
    j = tradier_get("/markets/quotes", {"symbols": ticker, "greeks": "false"})
    q = (j.get("quotes") or {}).get("quote")
    if isinstance(q, list) and q:
        q = q[0]
    if isinstance(q, dict):
        return q.get("last")
    return None

def get_expirations(ticker: str) -> list[str]:
    j = tradier_get("/markets/options/expirations", {"symbol": ticker, "includeAllRoots": "true", "strikes": "false"})
    dates = (j.get("expirations") or {}).get("date", [])
    if isinstance(dates, str):
        return [dates]
    return dates or []

def get_chain(ticker: str, expiration: str) -> list[dict]:
    j = tradier_get("/markets/options/chains", {"symbol": ticker, "expiration": expiration, "greeks": "false"})
    options = (j.get("options") or {}).get("option", [])
    if isinstance(options, dict):
        return [options]
    return options or []

# ----------------------------
# DB
# ----------------------------
def get_engine():
    return create_engine(PG_DSN, pool_pre_ping=True)

def ensure_schema(engine):
    ddl = """
    CREATE TABLE IF NOT EXISTS option_chain_eod (
        symbol           TEXT NOT NULL,
        quoteDate        DATE NOT NULL,
        underlyingLast   NUMERIC,
        expireDate       DATE NOT NULL,
        strike           NUMERIC NOT NULL,

        callSymbol       TEXT,
        callVolume       BIGINT,
        callBid          NUMERIC,
        callAsk          NUMERIC,
        callMid          NUMERIC,

        putSymbol        TEXT,
        putVolume        BIGINT,
        putBid           NUMERIC,
        putAsk           NUMERIC,
        putMid           NUMERIC,

        itmPercCalls     NUMERIC,
        itmPercPuts      NUMERIC,
        dte              INTEGER,

        PRIMARY KEY (quoteDate, symbol, expireDate, strike)
    );
    """
    with engine.begin() as conn:
        conn.execute(text(ddl))

def upsert_rows(engine, rows: list[dict]):
    if not rows:
        return

    cols = list(rows[0].keys())
    tuples = [tuple(r.get(c) for c in cols) for r in rows]

    conflict_cols = ("quoteDate", "symbol", "expireDate", "strike")
    update_cols = [c for c in cols if c not in conflict_cols]

    sql = f"""
    INSERT INTO option_chain_eod ({",".join(cols)})
    VALUES %s
    ON CONFLICT ({",".join(conflict_cols)})
    DO UPDATE SET
    """ + ", ".join(f"{c}=EXCLUDED.{c}" for c in update_cols)

    with engine.begin() as conn:
        raw = conn.connection
        with raw.cursor() as cur:
            execute_values(cur, sql, tuples, page_size=2000)

# ----------------------------
# DISK SNAPSHOTS (new)
# ----------------------------
def ensure_disk_schema(engine):
    ddl = """
    CREATE TABLE IF NOT EXISTS disk_usage_daily (
        captured_at        TIMESTAMPTZ PRIMARY KEY,
        root_path          TEXT NOT NULL,
        volume_path        TEXT NOT NULL,

        root_total_bytes   BIGINT NOT NULL,
        root_used_bytes    BIGINT NOT NULL,

        vol_total_bytes    BIGINT NOT NULL,
        vol_used_bytes     BIGINT NOT NULL
    );

    CREATE INDEX IF NOT EXISTS disk_usage_daily_captured_at_idx
    ON disk_usage_daily (captured_at);
    """
    with engine.begin() as conn:
        conn.execute(text(ddl))

def _fs_usage_bytes(path: str) -> tuple[int, int]:
    """
    Returns (total_bytes, used_bytes) for the filesystem containing 'path'
    using os.statvfs (no shell calls).
    """
    st = os.statvfs(path)
    total = st.f_frsize * st.f_blocks
    free = st.f_frsize * st.f_bfree
    used = total - free
    return int(total), int(used)

def record_disk_snapshot(engine, captured_at: datetime | None = None):
    """
    Inserts one snapshot row per *day*. If you run multiple times the same day,
    it will no-op (keeps the first row for that day).
    """
    ensure_disk_schema(engine)

    if captured_at is None:
        captured_at = datetime.now(timezone.utc)

    # One row per calendar day (UTC) to keep it simple and consistent.
    # If you prefer America/New_York, we can switch later.
    snap_day = captured_at.date()

    root_total, root_used = _fs_usage_bytes(ROOT_MOUNT_PATH)
    vol_total, vol_used = _fs_usage_bytes(VOLUME_MOUNT_PATH)

    sql = """
    INSERT INTO disk_usage_daily (
        captured_at, root_path, volume_path,
        root_total_bytes, root_used_bytes,
        vol_total_bytes,  vol_used_bytes
    )
    SELECT
        :captured_at, :root_path, :volume_path,
        :root_total_bytes, :root_used_bytes,
        :vol_total_bytes,  :vol_used_bytes
    WHERE NOT EXISTS (
        SELECT 1
        FROM disk_usage_daily
        WHERE captured_at::date = :snap_day
    );
    """

    with engine.begin() as conn:
        conn.execute(
            text(sql),
            {
                "captured_at": captured_at,
                "snap_day": snap_day,
                "root_path": ROOT_MOUNT_PATH,
                "volume_path": VOLUME_MOUNT_PATH,
                "root_total_bytes": root_total,
                "root_used_bytes": root_used,
                "vol_total_bytes": vol_total,
                "vol_used_bytes": vol_used,
            },
        )

    log.info(
        "DISK SNAPSHOT | day=%s | root=%s/%s bytes | vol=%s/%s bytes",
        snap_day.isoformat(),
        root_used, root_total,
        vol_used, vol_total,
    )

# ----------------------------
# UTIL
# ----------------------------
def chunked(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# ----------------------------
# MAIN
# ----------------------------
def run_eod():
    engine = get_engine()
    ensure_schema(engine)

    run_date = date.today()
    log.info("EOD RUN START | %s tickers | run_date=%s", len(TICKERS), run_date.isoformat())

    for tickers_batch in chunked(TICKERS, TICKERS_PER_BATCH):
        for ticker in tickers_batch:
            t0 = time.time()
            try:
                # 1) Underlying last via quotes (fast, 1 call per ticker)
                underlying_last = get_underlying_last(ticker)
                if underlying_last is None:
                    log.warning("%s: underlying last is None (quotes). Continuing.", ticker)

                # 2) Expirations
                expirations = get_expirations(ticker)
                if not expirations:
                    log.warning("%s: no expirations", ticker)
                    continue

                total_rows = 0

                # 3) Chains per expiration (this is the heavy part)
                for exp in expirations:
                    chain = get_chain(ticker, exp)
                    if not chain:
                        continue

                    exp_date = date.fromisoformat(exp)
                    dte = (exp_date - run_date).days

                    # Normalize strike to int(x1000) for correct matching & speed
                    calls = {}
                    puts = {}

                    for o in chain:
                        s = o.get("strike")
                        if s is None:
                            continue
                        strike_i = int(round(float(s) * 1000))

                        ot = o.get("option_type")
                        if ot == "call":
                            calls[strike_i] = o
                        elif ot == "put":
                            puts[strike_i] = o

                    strikes_i = set(calls) | set(puts)
                    if not strikes_i:
                        continue

                    rows = []
                    for strike_i in strikes_i:
                        strike = strike_i / 1000.0
                        call = calls.get(strike_i, {}) or {}
                        put = puts.get(strike_i, {}) or {}

                        callBid = call.get("bid")
                        callAsk = call.get("ask")
                        putBid = put.get("bid")
                        putAsk = put.get("ask")

                        callMid = (callBid + callAsk) / 2 if callBid is not None and callAsk is not None else None
                        putMid = (putBid + putAsk) / 2 if putBid is not None and putAsk is not None else None

                        itmPercCalls = (
                            ((underlying_last - strike) / strike) * 100
                            if underlying_last not in (None, 0) and strike not in (None, 0)
                            else None
                        )
                        itmPercPuts = (-itmPercCalls) if itmPercCalls is not None else None

                        rows.append({
                            "symbol": ticker,
                            "quoteDate": run_date,
                            "underlyingLast": underlying_last,
                            "expireDate": exp_date,
                            "strike": strike,

                            "callSymbol": call.get("symbol"),
                            "callVolume": call.get("volume"),
                            "callBid": callBid,
                            "callAsk": callAsk,
                            "callMid": callMid,

                            "putSymbol": put.get("symbol"),
                            "putVolume": put.get("volume"),
                            "putBid": putBid,
                            "putAsk": putAsk,
                            "putMid": putMid,

                            "itmPercCalls": itmPercCalls,
                            "itmPercPuts": itmPercPuts,
                            "dte": dte,
                        })

                    upsert_rows(engine, rows)
                    total_rows += len(rows)

                    if SLEEP_BETWEEN_EXPIRATIONS_SECONDS > 0:
                        time.sleep(SLEEP_BETWEEN_EXPIRATIONS_SECONDS)

                dt = time.time() - t0
                log.info("%s: saved %s rows in %.2fs", ticker, total_rows, dt)

            except Exception as e:
                log.exception("%s: failed: %s", ticker, e)

            if SLEEP_BETWEEN_TICKERS_SECONDS > 0:
                time.sleep(SLEEP_BETWEEN_TICKERS_SECONDS)

    log.info("EOD RUN COMPLETE")

    if SNAPSHOT_AFTER_EOD:
        try:
            record_disk_snapshot(engine)
        except Exception as e:
            log.exception("DISK SNAPSHOT AFTER EOD FAILED: %s", e)

def run_snapshot_only():
    engine = get_engine()
    record_disk_snapshot(engine)

if __name__ == "__main__":
    # Usage:
    #   python backend.py            -> run EOD ingest
    #   python backend.py snapshot   -> record disk snapshot only (for cron)
    import sys

    if len(sys.argv) > 1 and sys.argv[1].lower() == "snapshot":
        run_snapshot_only()
    else:
        run_eod()



